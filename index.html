<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chengfeng Zhao(Ëµµ‰πòÈ£é)</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="styles_responsive.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
  <link href="https://fonts.cdnfonts.com/css/optima" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.min.js"></script>
  <link rel="icon" type="image/png" href="images/android-chrome-512x512.png">

</head>

<body>
    <div class="container_onecolumn">
        <div class="container">
            <div class="item text">
              <p>
                <name>Chengfeng Zhao</name>
              </p>
              <p>
                I am currently a first-year Ph.D student at Intelligent Graphics Lab in HKUST (IGL-HKUST), supervised by <a href="https://liuyuan-pal.github.io/">Prof. Yuan Liu</a>. 
                Prior to this, I obtained my master and bachelor's degree from ShanghaiTech University, advised by <a href="http://www.xu-lan.com/">Prof. Lan Xu</a>. 
                I was also fortunate to work closely with <a href="https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl=en">Prof. Jingyi Yu</a> and <a href="http://yuexinma.me/">Prof. Yuexin Ma</a>.
              </p>
              <p>
                My research interests are in Computer Graphics and 3D Computer Vision, specifically including video generation, human motion synthesis, learning-based garment simulation, large models, etc.
              </p> 
              <p>
                <a href="mailto:chengfeng.zhao@connect.ust.hk"><i class="fa fa-paper-plane"></i>&nbsp&nbspEmail</a></a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=PKYWxA0AAAAJ&hl=en"><i class="ai ai-google-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Chengfeng-Zhao/66501065"><i class="ai ai-semantic-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/AfterJourney00"><i class="fa fa-github"></i>&nbsp&nbspGithub</a> &nbsp/&nbsp
                <a href="https://orcid.org/0000-0002-8649-7470"><i class="ai ai-orcid-square ai-fw"></i></i>&nbsp&nbspORCID</a>
                
              </p>
            </div>
 
            <div class="item image">
              <img class="profile-photo" alt="profile photo" src="images/profile.jpg">
            </div>
          </div>
          
          
            <div class="item text2">
            <heading>üìÉ Publications</heading>
            </div>

            <!-- <div class="container2">
              <div class="item image2">
                  <img src='images/unic.gif' width=180; height="auto">
              </div>
              <div class="item text2">
              <a href="https://igl-hkust.github.io/UNIC/">
                  <font color=#1772d0>  <papertitle>UNIC: Neural Garment Deformation Field for Real-time Clothed Character Animation<sup>üîó</sup></papertitle></font>
              </a>
              <br>
              <strong>Chengfeng Zhao</strong>, 
              <a href="https://github.com/jumbo-q">Junbo Qi</a>,
              <a href="https://frank-zy-dou.github.io/">Zhiyang Dou</a>,
              <a href="https://www.cs.cmu.edu/~minchenl/">Minchen Li</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
              <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a>,
              <a href="https://liuyuan-pal.github.io/">Yuan Liu</a>
              <br>
              <em>arXiv, 2025.</em>
              <br>
              <a href="https://igl-hkust.github.io/UNIC/">Project Page</a> /
              <a href="https://arxiv.org/abs/2312.08869">arXiv</a> /
              <a href="https://github.com/IGL-HKUST/UNIC">Code</a> /
              <a href="https://github.com/IGL-HKUST/UNIC">Dataset</a> /
              <a href="https://www.youtube.com/watch?v=MdG00uakBa8">Video</a> /
              <button id="bib_button" class="link", onclick="showBib('unic_bib')">BibTex</button>
              <div id='unic_bib' hidden>
              <pre><code>@InProceedings{zhao2024imhoi,
author    = {Zhao, Chengfeng and Zhang, Juze and Du, Jiashen and Shan, Ziwei and Wang, Junye and Yu, Jingyi and Wang, Jingya and Xu, Lan},
title     = {I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month     = {June},
year      = {2024},
pages     = {729-741}
}</code></pre>
              </div>
          </div>
        </div>

      <hr> -->

      <div class="container2">
        <div class="item image2">
            <img src='images/synchuman.gif' width=180; height="auto">
        </div>
        <div class="item text2">
        <a href="https://xishuxishu.github.io/SyncHuman.github.io/">
            <font color=#1772d0>  <papertitle>SyncHuman: Synchronizing 2D and 3D Diffusion Models for Single-view Human Reconstruction</papertitle></font><sup>üîó</sup>
        </a>
        <br>
        <a href="https://openreview.net/profile?id=~Wenyue_Chen1">Wenyue Chen</a>,
        <a href="https://penghtyx.github.io/yuki-lipeng/">Peng Li</a>,
        <a href="https://wangguandongzheng.github.io/">Wangguandong Zheng</a>,
        <strong>Chengfeng Zhao</strong>,
        <a href="https://openreview.net/profile?id=~Mengfei_Li2">Mengfei Li</a>,
        <a href="https://openreview.net/profile?id=~Yaolong_Zhu1">Yaolong Zhu</a>,
        <a href="https://frank-zy-dou.github.io/">Zhiyang Dou</a>,
        <a href="https://scholar.google.com/citations?user=CEEvb64AAAAJ&hl=en">Ronggang Wang</a>,
        <a href="https://liuyuan-pal.github.io/">Yuan Liu</a>
        <br>
        <em>The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS), 2025.</em>
        <br>
        <a href="https://xishuxishu.github.io/SyncHuman.github.io/">Project Page</a> /
        <a href="https://arxiv.org/abs/2510.07723">arXiv</a> /
        <a href="https://github.com/IGL-HKUST/SyncHuman">Code</a> /
        <!-- <a href="https://www.youtube.com/watch?v=MdG00uakBa8">Video</a> / -->
        <button id="bib_button" class="link", onclick="showBib('synchuman_bib')">BibTex</button>
        <div id='synchuman_bib' hidden>
        <pre><code>@article{chen2025synchuman,
  title={SyncHuman: Synchronizing 2D and 3D Diffusion Models for Single-view Human Reconstruction},
  author={Wenyue Chen, Peng Li, Wangguandong Zheng, Chengfeng Zhao, Mengfei Li, Yaolong Zhu, Zhiyang Dou, Ronggang Wang, Yuan Liu},
  journal={arXiv preprint arXiv:2510.07723},
  year={2025}
}</code></pre>
        </div>
    </div>
  </div>

<hr>

      <div class="highlightcontainer2">
        <div class="item image2">
            <img src='images/mojito.gif' width=180; height="auto">
        </div>
        <div class="item text2">
        <a href="https://koyui.github.io/mojito/">
            <font color=#1772d0>  <papertitle>Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens<sup>üîó</sup></papertitle></font>
        </a>
        <br>
        <a href="https://cunkaixin.netlify.app/">Ziwei Shan*</a>,
        <a href="https://tropinoneh.github.io/profile/">Yaoyu He*</a>,
        <a href="https://alt-js.github.io/">Jiashen Du*</a>,
        <strong>Chengfeng Zhao<sup>&dagger;</sup></strong>, 
        <a href="https://zhanglele12138.github.io/">Jingyan Zhang</a>,
        <a href="http://www.xu-lan.com/">Lan Xu</a>
        <br>
        <em>arXiv, 2025.</em>
        <br>
        <a href="https://koyui.github.io/mojito/">Project Page</a> /
        <a href="https://arxiv.org/abs/2502.16175">arXiv</a> /
        <a href="https://github.com/koyui/mojito">Code</a> /
        <!-- <a href="https://www.youtube.com/watch?v=MdG00uakBa8">Video</a> / -->
        <button id="bib_button" class="link", onclick="showBib('mojito_bib')">BibTex</button>
        <div id='mojito_bib' hidden>
        <pre><code>@article{shan2025mojito,
  title   = {Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens},
  author  = {Shan, Ziwei and He, Yaoyu and Du, Jiashen and Zhao, Chengfeng and Zhang, Jingyan and Xu, Lan},
  journal = {arXiv preprint arXiv:2502.16175},
  year    = {2025}
}</code></pre>
        </div>
    </div>
  </div>

<hr>

      <div class="container2">
        <div class="item image2">
            <img src='images/4drecon_survey.png' width=180; height="auto">
        </div>
        <div class="item text2">
        <a href="https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence">
            <font color=#1772d0>  <papertitle>Reconstructing 4D Spatial Intelligence: A Survey<sup>üîó</sup></papertitle></font>
        </a>
        <br>
        <a href="https://yukangcao.github.io/">Yukang Cao</a>,
        <a href="https://scholar.google.com/citations?user=cRpteW4AAAAJ&hl=en">Jiahao Lu</a>,
        <a href="https://www.linkedin.com/in/zhisheng-huang-930357342/">Zhisheng Huang</a>,
        <a href="https://mickshen7558.github.io/">Zhuowen Shen</a>,
        <strong>Chengfeng Zhao</strong>, 
        <a href="https://hongfz16.github.io/">Fangzhou Hong</a>,
        <a href="https://frozenburning.github.io/">Zhaoxi Chen</a>,
        <a href="https://people.tamu.edu/~xinli/">Xin Li</a>,
        <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a>,
        <a href="https://liuyuan-pal.github.io/">Yuan Liu</a>,
        <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
        <br>
        <em>arXiv, 2025.</em>
        <br>
        <a href="https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence">Project Page</a> /
        <a href="https://arxiv.org/abs/2507.21045">arXiv</a> /
        <a href="https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence">Code</a> /
        <button id="bib_button" class="link", onclick="showBib('4dsurvey_bib')">BibTex</button>
        <div id='4dsurvey_bib' hidden>
        <pre><code>@article{cao2025reconstructing,
  title={Reconstructing 4d spatial intelligence: A survey},
  author={Cao, Yukang and Lu, Jiahao and Huang, Zhisheng and Shen, Zhuowen and Zhao, Chengfeng and Hong, Fangzhou and Chen, Zhaoxi and Li, Xin and Wang, Wenping and Liu, Yuan and others},
  journal={arXiv preprint arXiv:2507.21045},
  year={2025}
}</code></pre>
        </div>
    </div>
  </div>

<hr>

            <div class="highlightcontainer2">
                <div class="item image2">
                    <img src='images/imhoi.gif' width=180; height="auto">
                </div>
                <div class="item text2">
                <a href="https://afterjourney00.github.io/IM-HOI.github.io/">
                    <font color=#1772d0>  <papertitle>I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions<sup>üîó</sup></papertitle></font>
                </a>
                <br>
                <strong>Chengfeng Zhao</strong>, 
                <a href="https://juzezhang.github.io/">Juze Zhang</a>,
                <a href="https://alt-js.github.io/">Jiashen Du</a>,
                <a href="https://cunkaixin.netlify.app/">Ziwei Shan</a>,
                Junye Wang,
                <a href=https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl=en>Jingyi Yu</a>,
                <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
                <a href="http://www.xu-lan.com/">Lan Xu</a>
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</em>
                <br>
                <a href="https://afterjourney00.github.io/IM-HOI.github.io/">Project Page</a> /
                <a href="https://arxiv.org/abs/2312.08869">arXiv</a> /
                <a href="https://github.com/AfterJourney00/IMHD-Dataset">Code</a> /
                <a href="https://forms.gle/3MDh3b4szhFwcYa26">Dataset</a> /
                <a href="https://www.youtube.com/watch?v=MdG00uakBa8">Video</a> /
                <button id="bib_button" class="link", onclick="showBib('imhoi_bib')">BibTex</button> &nbsp;
                <font style="color:#f09228;">(Welcome to participate in</font> <a href="https://rhobin-challenge.github.io/">3rd Rhobin Challenge</a><font style="color:#f09228;">!)</font>
                <div id='imhoi_bib' hidden>
                <pre><code>@inproceedings{zhao2024imhoi,
  title={I'm hoi: Inertia-aware monocular capture of 3d human-object interactions},
  author={Zhao, Chengfeng and Zhang, Juze and Du, Jiashen and Shan, Ziwei and Wang, Junye and Yu, Jingyi and Wang, Jingya and Xu, Lan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={729--741},
  year={2024}
}</code></pre>
                </div>
            </div>
          </div>

        <hr>

          <div class="container2">
            <div class="item image2">
              <img src='images/hoim3.gif' width=180; height="auto">
            </div>
            <div class="item text2">
              <a href="https://juzezhang.github.io/HOIM3_ProjectPage/">
                <font color=#1772d0>  <papertitle>HOI-M<sup>3</sup>: Capture Multiple Humans and Objects Interaction within Contextual Environment<sup>üîó</sup></papertitle></font>
                </a>
                <br>
                <a href="https://juzezhang.github.io/">Juze Zhang*</a>,
                <a href="https://zhanglele12138.github.io/">Jingyan Zhang*</a>,
                Zining Song,
                Zhanhe Shi,
                <strong>Chengfeng Zhao</strong>,
                <a href="https://shiye21.github.io/">Ye Shi</a>,
                <a href=https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl=en>Jingyi Yu</a>,
                <a href="http://www.xu-lan.com/">Lan Xu</a>,
                <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</em>
                <br>
                <a href="https://juzezhang.github.io/HOIM3_ProjectPage/">Project Page</a> /
                <a href="https://arxiv.org/abs/2404.00299">arXiv</a> /
                <a href="https://drive.google.com/drive/folders/1bT7J0XnbUx5goixgJRWJxpycOFffpwOc?usp=sharing">Dataset</a> /
                <a href="https://www.youtube.com/watch?v=Fq6iqoXC99A">Video</a> /
                <button id="bib_button" class="link", onclick="showBib('hoim3_bib')">BibTex</button>
                <div id='hoim3_bib' hidden>
                <pre><code>@InProceedings{Zhang_2024_CVPR,
  author    = {Zhang, Juze and Zhang, Jingyan and Song, Zining and Shi, Zhanhe and Zhao, Chengfeng and Shi, Ye and Yu, Jingyi and Xu, Lan and Wang, Jingya},
  title     = {HOI-M{\textasciicircum}3: Capture Multiple Humans and Objects Interaction within Contextual Environment},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2024},
  pages     = {516-526}
}</code></pre>
            </div>
        </div>
      </div>

      <hr>

      <div class="container2">
        <div class="item image2">
          <img src='images/livehps.gif' width=180; height="auto">
        </div>
        <div class="item text2">
          <a href="https://arxiv.org/abs/2402.17171">
            <font color=#1772d0>  <papertitle>LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment<sup>üîó</sup></papertitle></font>
          </a>
          <br>
          <a href=https://ren-ym.github.io/>Yiming Ren</a>, 
          Xiao Han,
          <strong>Chengfeng Zhao</strong>,
          <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
          <a href="http://www.xu-lan.com/">Lan Xu</a>,
          <a href=https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl=en>Jingyi Yu</a>,
          <a href=http://yuexinma.me/>Yuexin Ma</a>
          <br>
          <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</em>
          <br>
          <a href="https://arxiv.org/abs/2402.17171">arXiv</a> /
          <button id="bib_button" class="link", onclick="showBib('livehps_bib')">BibTex</button>
          <div id='livehps_bib' hidden>
          <pre><code>@InProceedings{Ren_2024_CVPR,
  author    = {Ren, Yiming and Han, Xiao and Zhao, Chengfeng and Wang, Jingya and Xu, Lan and Yu, Jingyi and Ma, Yuexin},
  title     = {LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2024},
  pages     = {1281-1291}
}</code></pre>
        </div>
    </div>
  </div>

<hr>

<div class="highlightcontainer2">
<div class="item image2">
  <img src='images/lip.gif' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://4dvlab.github.io/project_page/LIPD.html">
    <font color=#1772d0>  <papertitle>LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse Inertial and LiDAR Sensors<sup>üîó</sup></papertitle></font>
  <br>
  <a href=https://ren-ym.github.io/>Yiming Ren*</a>, 
  <strong>Chengfeng Zhao*</strong>,
  <a href="https://virtualhumans.mpi-inf.mpg.de/people/He.html">Yannan He</a>,
  <a href="https://coralemon.github.io/">Peishan Cong</a>,
  <a href="https://tr3e.github.io/">Han Liang</a>,
  <a href=https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl=en>Jingyi Yu</a>,
  <a href="http://www.xu-lan.com/">Lan Xu</a>,
  <a href=http://yuexinma.me/>Yuexin Ma</a>
  <br>
  <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VR), 2023.</em>
  <br>
  <a href="https://4dvlab.github.io/project_page/LIPD.html">Project Page</a>
  / <a href="https://arxiv.org/abs/2205.15410">arXiv</a>
  / <a href="https://github.com/4DVLab/LIP">Code</a>
  / <a href="https://drive.google.com/file/d/1SVO77FWFUOtC-Et2sdlgAdiNQWzgzxt0/view?usp=sharing">Dataset</a>
  / <a href="https://www.youtube.com/watch?v=ao9eTGPQT6k">Video</a>
  / <button id="bib_button" class="link", onclick="showBib('lip_bib')">BibTex</button>
  <div id='lip_bib' hidden>
  <pre><code>@article{ren2023lidar,
  title={Lidar-aid inertial poser: Large-scale human motion capture by sparse inertial and lidar sensors},
  author={Ren, Yiming and Zhao, Chengfeng and He, Yannan and Cong, Peishan and Liang, Han and Yu, Jingyi and Xu, Lan and Ma, Yuexin},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={29},
  number={5},
  pages={2337--2347},
  year={2023},
  publisher={IEEE}
}</code></pre>
  </div>
  </div>
</div>

<hr>

<div class="container2">
<div class="item image2">
  <img src='images/hybridcap.gif' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://arxiv.org/abs/2203.09287">
    <font color=#1772d0>  <papertitle>HybridCap: Inertia-aid Monocular Capture of Challenging Human Motions<sup>üîó</sup></papertitle></font>
  </a>
  <br>
  <a href="https://tr3e.github.io/">Han Liang</a>,
  <a href="https://virtualhumans.mpi-inf.mpg.de/people/He.html">Yannan He</a>,
  <strong>Chengfeng Zhao</strong>, 
  Mutian Li,
  <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
  <a href=https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl=en>Jingyi Yu</a>,
  <a href="http://www.xu-lan.com/">Lan Xu</a>
  <br>
  <em>The 37th AAAI Conference on Artificial Intelligence (AAAI), 2023.</em>
  <br>
  <a href="https://arxiv.org/abs/2203.09287">arXiv</a> /
  <a href="https://www.youtube.com/watch?v=fV4IRTUdSws">Video</a> / 
  <button id="bib_button" class="link", onclick="showBib('hybridcap_bib')">BibTex</button>
  <div id='hybridcap_bib' hidden>           
    <pre><code>@inproceedings{liang2023hybridcap,
  title={Hybridcap: Inertia-aid monocular capture of challenging human motions},
  author={Liang, Han and He, Yannan and Zhao, Chengfeng and Li, Mutian and Wang, Jingya and Yu, Jingyi and Xu, Lan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={2},
  pages={1539--1548},
  year={2023}
}</code></pre>
  </div>
</div>
</div>

<!-- <div class="item text2">
  <heading>üèÜ Hornors and Awards</heading>

</div> -->

<div class="item text2">
  <heading>üíª Internships</heading>
</div>
<div class="container2">
  <div class="item image2">
    <img src='images/hunyuan.png' width=180; height="auto">
  </div>
  <div class="item text2">
    <b>Digital Human Algorithm Researcher</b>
    <br>
    Tencent Hunyuan, Shenzhen, China
    <br>
    Mentor: <a href="https://chingswy.github.io/">Qing Shuai</a>
    <br>
    2025.08 - now
  </div>
</div>

<div class="container2">
  <div class="item image2">
    <img src='images/SenseTime_logo.png' width=180; height="auto">
  </div>
  <div class="item text2">
    <b>Computer Vision Researcher</b>
    <br>
    SenseTime, Shanghai, China
    <br>
    Mentor: <a href="https://scholar.google.com/citations?user=F5rVlz0AAAAJ&hl=en">Cheng Li</a>
    <br>
    2021.06 - 2021.09
  </div>
</div>

<div class="item text2">
  <heading>	üìå Academic Services</heading>
  <ul>
    <li><a href="https://rhobin-challenge.github.io/">3rd Rhobin Challenge</a>, <b>co-organizer</b></li>
    <li>SIGGRAPH / CVPR / AAAI, <b>reviewer</b></li>
  </ul>
</div>

<div class="item text2">
  <heading>üìã Teaching Assistants</heading>
  <ul>
    <li>2025 Fall, <b>MTLE5110: Product Development and Prototyping (AI Workshop)</b>, HKUST</li>
    <li>2023 Fall & 2022 Fall, <b>CS280: Deep Learning</b>, ShanghaiTech University</li>
    <li>2022 Spring, <b>CS100: Introduction to Programming</b>, ShanghaiTech University</li>
    <li>2021 Fall, <b>CS130: Operating Systems</b>, ShanghaiTech University</li>
    <li>2021 Spring, <b>SI100B: Introduction to Information Science and Technology</b>, ShanghaiTech University</li>
  </ul>
</div>

<div class="item text2">
  <heading>üéì Educations</heading>
</div>

<div class="container2">
  <div class="item image2">
      <img src='images/HKUST.png' width=180; height="auto">
  </div>
  <div class="item text2">
  <b>Ph.D</b>
  <br>
  The Hong Kong University of Science and Technology, Hong Kong, China
  <br>
  2025.09 - now
  </div>
</div>

<div class="container2">
  <div class="item image2">
      <img src='images/shanghaitech.png' width=180; height="auto">
  </div>
  <div class="item text2">
  <b>Master & Bachelor</b>
  <br>
  ShanghaiTech University, Shanghai, China
  <br>
  <i>Outstanding Graduate of Shanghai</i> / <i>Outstanding Graduate of ShanghaiTech University</i>
  <br>
  2018.09 - 2025.06
<!--   <ul>
    <li>Graduate:</li>
    <ul>
      <li>2025.04 <b>‚ÄúOutstanding Graduate of ShanghaiTech University‚Äù</b></li>
      <li>2024.12 & 2023.12 <b>‚ÄúMerit Student‚Äù</b></li>
      <li>2024.03 <b>‚ÄúOutstanding Teaching Assistant‚Äù</b></li>
    </ul>
    <li>Undergraduate:</li>
    <ul>
      <li>2022.06 <b>‚ÄúOutstanding Graduate of Shanghai‚Äù</b></li>
      <li>2022.06 <b>‚ÄúOutstanding Graduate of ShanghaiTech University‚Äù</b></li>
      <li>2021.12 <b>‚ÄúOutstanding Student Leader‚Äù</b></li>
      <li>2021.12 <b>‚ÄúOutstanding Student‚Äù</b></li>
      <li>2020.12 <b>‚ÄúMerit Student‚Äù</b></li>
    </ul>
  </ul> -->
  </div>
</div>

<div class="container2">
  <div class="item image2">
      <img src='images/nsfz.jpg' width=180; height="auto">
  </div>
  <div class="item text2">
  <b>High school</b>
  <br>
  High School Affiliated to Nanjing Normal University, Nanjing, China
  <br>
  2015.09 - 2018.06
  </div>
</div>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <p style="text-align:right;">
         Template from <a href="https://qianlim.github.io/">this website</a>.
      </p>
    </td>
  </tr>
</tbody></table>

<script type="text/javascript" src="show_bib.js"></script>
</body>
</html>